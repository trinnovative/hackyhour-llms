{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:22.686250800Z",
     "start_time": "2023-10-31T10:27:22.617244400Z"
    }
   },
   "id": "af6a1b4b9056f760"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LangChain\n",
    "- Open-source Framework\n",
    "- Python, JS/TS\n",
    "- Kombination von LLMs mit externen Funktionalitäten und Daten"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "387bdd6d05b1275a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model I/O\n",
    "\n",
    "### Prompts\n",
    "Die Anweisung/Frage/Aufgabestellung die das Modell bearbeiten soll.\n",
    "\n",
    "### Language Models\n",
    "Interfaces und Integrationen von verschiedenen Modellen\n",
    "- LLMs: input text → output text (z.B. `text-davinci-003`)\n",
    "- Chat models: input Liste von Chatnachrichten (System, Human, AI) → output Chatnachricht (z.B. `gpt-3.5-turbo`)\n",
    "\n",
    "### Output Parsers\n",
    "Wandeln die Textausgabe der Modelle in strukturierte Daten um."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e82bd7bdd6e6b99"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant who generates comma separated lists. A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. ONLY return a comma separated list, and nothing more.'), HumanMessage(content='programming languages')])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = (\"You are a helpful assistant who generates comma separated lists. \"\n",
    "                   \"A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. \"\n",
    "                   \"ONLY return a comma separated list, and nothing more.\")\n",
    "human_template = \"{text}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "prompt_template.invoke({\"text\": \"programming languages\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:23.042939400Z",
     "start_time": "2023-10-31T10:27:22.627718600Z"
    }
   },
   "id": "476618f2b18ea11b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='Hello! How can I assist you today?')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# default: gpt-3.5-turbo\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "llm.invoke(\"hi!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:24.213666300Z",
     "start_time": "2023-10-31T10:27:23.014248700Z"
    }
   },
   "id": "a8f9650d71065b79"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['python', 'typescript']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "parser.invoke(\"python, typescript\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:24.281845400Z",
     "start_time": "2023-10-31T10:27:24.205619900Z"
    }
   },
   "id": "6e83e45ece34069b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chains\n",
    "Chains erlauben es mehrere Modelle (LLM, Chat) und Komponenten (Prompt, Parser, Retriever) zu verknüpfen.\n",
    "Für das Erstellen von Chains wird vorrangig die [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/) verwendet.\n",
    "\n",
    "```python\n",
    "chain = prompt | model | parser\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e5c0d2fa441f23e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['Python', 'Java', 'C++', 'JavaScript', 'Ruby']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | llm | parser\n",
    "\n",
    "chain.invoke({\"text\": \"programming languages\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:25.387820600Z",
     "start_time": "2023-10-31T10:27:24.233259700Z"
    }
   },
   "id": "5f727cd73d7a8e80"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Memory\n",
    "- Informationen aus vorausgegangenen Konversationen speichern\n",
    "- Speichern und abrufen von Informationen (reading/writing)\n",
    "- Mit Tokenlimit umgehen"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e250ac5d8db585f"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True    \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:25.821222800Z",
     "start_time": "2023-10-31T10:27:25.388760100Z"
    }
   },
   "id": "411c67a682be4716"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hi! My name is Dave\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input': 'hi! My name is Dave',\n 'history': '',\n 'response': \"Hello Dave! It's nice to meet you. How can I assist you today?\"}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\"hi! My name is Dave\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:26.815909100Z",
     "start_time": "2023-10-31T10:27:25.822236400Z"
    }
   },
   "id": "991584c565d5f45b"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hi! My name is Dave\n",
      "AI: Hello Dave! It's nice to meet you. How can I assist you today?\n",
      "Human: What is my name?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input': 'What is my name?',\n 'history': \"Human: hi! My name is Dave\\nAI: Hello Dave! It's nice to meet you. How can I assist you today?\",\n 'response': 'Your name is Dave.'}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\"What is my name?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:27.671920Z",
     "start_time": "2023-10-31T10:27:26.816866400Z"
    }
   },
   "id": "e53594d26eb47c07"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[HumanMessage(content='hi! My name is Dave'),\n AIMessage(content=\"Hello Dave! It's nice to meet you. How can I assist you today?\"),\n HumanMessage(content='What is my name?'),\n AIMessage(content='Your name is Dave.')]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.memory.chat_memory.messages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:27.672941800Z",
     "start_time": "2023-10-31T10:27:27.653841300Z"
    }
   },
   "id": "717ee77c618fd524"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retrieval\n",
    "- Einbindung von Daten, die nicht Teil des Trainingssets des Modells sind\n",
    "- Retrieval Augmented Generation (RAG) \n",
    "- Dokumente laden/transformieren, Embeddings, Vector Stores, Retrievers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5396deb3f3274e68"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:27.769992Z",
     "start_time": "2023-10-31T10:27:27.661852500Z"
    }
   },
   "id": "910c614705506fc7"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts([\"Hacky Hour: Large Language Models for Developers \"\n",
    "                                \"anwenden, integrieren und erweitern, \"\n",
    "                                \"Degginger Regensburg, 14.11.2023, Einlass 18:00 Uhr, Start 18:30 Uhr\"], embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:28.337901Z",
     "start_time": "2023-10-31T10:27:27.772902300Z"
    }
   },
   "id": "bb51de2626858328"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'Die Hacky Hour findet am 14.11.2023 in Degginger Regensburg statt.'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Wann und wo findet die Hacky Hour statt?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:29.873716400Z",
     "start_time": "2023-10-31T10:27:28.337901Z"
    }
   },
   "id": "7f1f57bc55d4996c"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'Die Hacky Hour startet um 18:30 Uhr.'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Um wie viel Uhr Startet die Hacky Hour?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:31.118452600Z",
     "start_time": "2023-10-31T10:27:29.866657700Z"
    }
   },
   "id": "188c38a026e3aedf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agents\n",
    "- Agents erlauben LLMs mit ihrer Umgebung zu interagieren (Funktionen, APIs).\n",
    "- LLMs werden als Reasoning-Engine genutzt um zu entscheiden welche Aktionen ausgeführt werden sollen (Chains sind hardcoded)\n",
    "- AgentAction: Welche Aktion soll ausgeführt werden (tool, tool_input)\n",
    "- AgentFinish: Ist der Agent mit seiner Arbeit fertig und soll das Ergebnis zurückgeben\n",
    "- intermediate_steps: Welche Schritte wurden bereits ausgeführt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac938ff65cd88e25"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType, load_tools\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:31.195505300Z",
     "start_time": "2023-10-31T10:27:31.105434200Z"
    }
   },
   "id": "f77b98fb97c36bcb"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-0613\", temperature=0)\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, \n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:27:31.212786Z",
     "start_time": "2023-10-31T10:27:31.196477600Z"
    }
   },
   "id": "2e9b0da2a78d57a8"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m```json\n",
      "{\n",
      "    \"action\": \"Search\",\n",
      "    \"action_input\": \"Leonardo DiCaprio's current girlfriend\"\n",
      "}\n",
      "```\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3mLeonardo DiCaprio has moved on from his months-long fling with Gigi Hadid with Italian model Vittoria Ceretti. The globe-trotting couple have been spotted showing PDA all over the world since first sparking romance rumors in August 2023 with an Ibiza nightclub makeout session.\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m```json\n",
      "{\n",
      "    \"action\": \"Search\",\n",
      "    \"action_input\": \"Vittoria Ceretti age\"\n",
      "}\n",
      "```\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3m25 years\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m```json\n",
      "{\n",
      "    \"action\": \"Calculator\",\n",
      "    \"action_input\": \"25 * 3\"\n",
      "}\n",
      "```\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mAnswer: 75\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Leonardo DiCaprio's current girlfriend is Italian model Vittoria Ceretti, who is 25 years old. When her age is multiplied by 3, the result is 75.\"\n",
      "}\n",
      "```\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input': 'Who is leonardo di caprios current girlfriend and what is here age multiplied by 3?',\n 'chat_history': [HumanMessage(content='Who is leonardo di caprios current girlfriend and what is here age multiplied by 3?'),\n  AIMessage(content=\"Leonardo DiCaprio's current girlfriend is Italian model Vittoria Ceretti, who is 25 years old. When her age is multiplied by 3, the result is 75.\")],\n 'output': \"Leonardo DiCaprio's current girlfriend is Italian model Vittoria Ceretti, who is 25 years old. When her age is multiplied by 3, the result is 75.\"}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.invoke(\"Who is leonardo di caprios current girlfriend and what is here age multiplied by 3?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:28:04.196400600Z",
     "start_time": "2023-10-31T10:27:31.214684900Z"
    }
   },
   "id": "b08a8f0b88e972f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OpenAI function with LangChain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0746556627ee522"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:28:04.196400600Z",
     "start_time": "2023-10-31T10:28:04.089435700Z"
    }
   },
   "id": "3b7fac4b8bd27193"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import StructuredTool\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0)\n",
    "\n",
    "class TemperatureScale(str, Enum):\n",
    "    CELSIUS = \"celsius\"\n",
    "    FAHRENHEIT = \"fahrenheit\"\n",
    "\n",
    "class WeatherInput(BaseModel):\n",
    "    location: str = Field(description=\"The city and state, e.g. San Francisco, CA\")\n",
    "    unit: TemperatureScale = Field(default=TemperatureScale.FAHRENHEIT)\n",
    "\n",
    "\n",
    "tools = [\n",
    "    StructuredTool(\n",
    "        name=\"get_current_weather\",\n",
    "        func=get_current_weather,\n",
    "        description=\"Get the current weather in a given location\",\n",
    "        args_schema=WeatherInput\n",
    "    )\n",
    "]\n",
    "\n",
    "agent_executor = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.OPENAI_FUNCTIONS, \n",
    "    verbose=True, \n",
    "    max_execution_time=30\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:28:04.196400600Z",
     "start_time": "2023-10-31T10:28:04.095047700Z"
    }
   },
   "id": "123013e1d2313ce6"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `get_current_weather` with `{'location': 'Boston, MA'}`\n",
      "\n",
      "\n",
      "\u001B[0m\u001B[36;1m\u001B[1;3m{\"location\": \"Boston, MA\", \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}\u001B[0m\u001B[32;1m\u001B[1;3mThe current weather in Boston, MA is 72°F and it is sunny and windy.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input': \"What's the weather like in Boston?\",\n 'output': 'The current weather in Boston, MA is 72°F and it is sunny and windy.'}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\"What's the weather like in Boston?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T10:30:59.621925200Z",
     "start_time": "2023-10-31T10:30:57.483947Z"
    }
   },
   "id": "2d4ba39327567774"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
