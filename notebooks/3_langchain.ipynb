{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T16:27:19.482228700Z",
     "start_time": "2023-10-24T16:27:19.474310Z"
    }
   },
   "id": "af6a1b4b9056f760"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LangChain\n",
    "- Open-source Framework\n",
    "- Python, JS/TS\n",
    "- Kombination von LLMs mit externen Funktionalitäten und Daten"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "387bdd6d05b1275a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model I/O\n",
    "\n",
    "### Prompts\n",
    "Die Anweisung/Frage/Aufgabestellung die das Modell bearbeiten soll.\n",
    "\n",
    "### Language Models\n",
    "Interfaces und Integrationen von verschiedenen Modellen\n",
    "- LLMs: input text → output text (z.B. `text-davinci-003`)\n",
    "- Chat models: input Liste von Chatnachrichten (System, Human, AI) → output Chatnachricht (z.B. `gpt-3.5-turbo`)\n",
    "\n",
    "### Output Parsers\n",
    "Wandeln die Textausgabe der Modelle in strukturierte Daten um."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e82bd7bdd6e6b99"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant who generates comma separated lists. A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. ONLY return a comma separated list, and nothing more.'), HumanMessage(content='programming languages')])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = (\"You are a helpful assistant who generates comma separated lists. \"\n",
    "                   \"A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. \"\n",
    "                   \"ONLY return a comma separated list, and nothing more.\")\n",
    "human_template = \"{text}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "prompt_template.invoke({\"text\": \"programming languages\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T14:42:27.005977300Z",
     "start_time": "2023-10-24T14:42:26.997242300Z"
    }
   },
   "id": "476618f2b18ea11b"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='Hello! How can I assist you today?')"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# default: gpt-3.5-turbo\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"hi!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T14:42:29.406908Z",
     "start_time": "2023-10-24T14:42:27.845409500Z"
    }
   },
   "id": "a8f9650d71065b79"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "['python', 'typescript']"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "parser.invoke(\"python, typescript\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T14:42:31.013517600Z",
     "start_time": "2023-10-24T14:42:31.003461500Z"
    }
   },
   "id": "6e83e45ece34069b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chains\n",
    "Chains erlauben es mehrere Modelle (LLM, Chat) und Komponenten (Prompt, Parser, Retriever) zu verknüpfen.\n",
    "Für das Erstellen von Chains wird vorrangig die [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/) verwendet.\n",
    "\n",
    "```python\n",
    "chain = prompt | model | parser\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e5c0d2fa441f23e"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "['Python', 'Java', 'JavaScript', 'C++', 'Ruby']"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | llm | parser\n",
    "\n",
    "chain.invoke({\"text\": \"programming languages\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T14:42:32.574481700Z",
     "start_time": "2023-10-24T14:42:31.621631100Z"
    }
   },
   "id": "5f727cd73d7a8e80"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Memory\n",
    "- Informationen aus vorausgegangenen Konversationen speichern\n",
    "- Speichern und abrufen von Informationen (reading/writing)\n",
    "- Mit Tokenlimit umgehen"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e250ac5d8db585f"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "memory = ConversationBufferMemory()\n",
    "conversation_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True    \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T14:42:34.784041100Z",
     "start_time": "2023-10-24T14:42:34.780302600Z"
    }
   },
   "id": "411c67a682be4716"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hi! My name is Dave\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input': 'hi! My name is Dave',\n 'history': '',\n 'response': \"Hello Dave! It's a pleasure to meet you. How can I assist you today?\"}"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\"hi! My name is Dave\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T11:50:26.460856700Z",
     "start_time": "2023-10-20T11:50:24.681992700Z"
    }
   },
   "id": "991584c565d5f45b"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hi! My name is Dave\n",
      "AI: Hello Dave! It's a pleasure to meet you. How can I assist you today?\n",
      "Human: What is my name?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input': 'What is my name?',\n 'history': \"Human: hi! My name is Dave\\nAI: Hello Dave! It's a pleasure to meet you. How can I assist you today?\",\n 'response': 'Your name is Dave.'}"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\"What is my name?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T11:50:26.854210300Z",
     "start_time": "2023-10-20T11:50:26.460856700Z"
    }
   },
   "id": "e53594d26eb47c07"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "[HumanMessage(content='hi! My name is Dave'),\n AIMessage(content=\"Hello Dave! It's a pleasure to meet you. How can I assist you today?\"),\n HumanMessage(content='What is my name?'),\n AIMessage(content='Your name is Dave.')]"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.memory.chat_memory.messages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T11:50:54.674312600Z",
     "start_time": "2023-10-20T11:50:54.671054700Z"
    }
   },
   "id": "717ee77c618fd524"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retrieval\n",
    "- Einbindung von Daten, die nicht Teil des Trainingssets des Modells sind\n",
    "- Retrieval Augmented Generation (RAG) \n",
    "- Dokumente laden/transformieren, Embeddings, Vector Stores, Retrievers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5396deb3f3274e68"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T14:44:11.728101500Z",
     "start_time": "2023-10-24T14:44:11.719581300Z"
    }
   },
   "id": "910c614705506fc7"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts([\"Hacky Hour: Large Language Models for Developers \"\n",
    "                                \"anwenden, integrieren und erweitern, \"\n",
    "                                \"Degginger Regensburg, 14.11.2023, Einlass 18:00 Uhr, Start 18:30 Uhr\"], embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T14:44:14.055075Z",
     "start_time": "2023-10-24T14:44:13.756225100Z"
    }
   },
   "id": "bb51de2626858328"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "'Die Hacky Hour findet am 14.11.2023 in Degginger Regensburg statt.'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Wann und wo findet die Hacky Hour statt?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-20T11:18:40.660326700Z"
    }
   },
   "id": "7f1f57bc55d4996c"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "'Die Hacky Hour startet um 18:30 Uhr.'"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Um wie viel Uhr Startet die Hacky Hour?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T14:44:19.986964100Z",
     "start_time": "2023-10-24T14:44:18.076888800Z"
    }
   },
   "id": "188c38a026e3aedf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agents\n",
    "- Agents erlauben LLMs mit ihrer Umgebung zu interagieren (Funktionen, APIs).\n",
    "- LLMs werden als Reasoning-Engine genutzt um zu entscheiden welche Aktionen ausgeführt werden sollen (Chains sind hardcoded)\n",
    "- AgentAction: Welche Aktion soll ausgeführt werden (tool, tool_input)\n",
    "- AgentFinish: Ist der Agent mit seiner Arbeit fertig und soll das Ergebnis zurückgeben\n",
    "- intermediate_steps: Welche Schritte wurden bereits ausgeführt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac938ff65cd88e25"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType, load_tools\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T15:06:49.890329Z",
     "start_time": "2023-10-26T15:06:49.875054200Z"
    }
   },
   "id": "f77b98fb97c36bcb"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-0613\")\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, \n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T16:05:11.483039700Z",
     "start_time": "2023-10-26T16:05:11.463978400Z"
    }
   },
   "id": "2e9b0da2a78d57a8"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m```json\n",
      "{\n",
      "    \"action\": \"Search\",\n",
      "    \"action_input\": \"Leonardo DiCaprio's current girlfriend\"\n",
      "}\n",
      "```\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3mLeonardo DiCaprio and new girlfriend Vittoria Ceretti hit another Paris party.\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m```json\n",
      "{\n",
      "    \"action\": \"Search\",\n",
      "    \"action_input\": \"Vittoria Ceretti's age\"\n",
      "}\n",
      "```\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3m25 years\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m```json\n",
      "{\n",
      "    \"action\": \"Calculator\",\n",
      "    \"action_input\": \"25 * 3\"\n",
      "}\n",
      "```\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mAnswer: 75\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Leonardo DiCaprio's current girlfriend is Vittoria Ceretti, and her age multiplied by 3 is 75.\"\n",
      "}\n",
      "```\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input': 'Who is leonardo di caprios current girlfriend and what is here age multiplied by 3?',\n 'chat_history': [HumanMessage(content='Who is leonardo di caprios current girlfriend and what is here age multiplied by 3?'),\n  AIMessage(content=\"Leonardo DiCaprio's current girlfriend is Vittoria Ceretti, and her age multiplied by 3 is 75.\")],\n 'output': \"Leonardo DiCaprio's current girlfriend is Vittoria Ceretti, and her age multiplied by 3 is 75.\"}"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.invoke(\"Who is leonardo di caprios current girlfriend and what is here age multiplied by 3?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T16:06:05.777106300Z",
     "start_time": "2023-10-26T16:05:38.496070Z"
    }
   },
   "id": "b08a8f0b88e972f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OpenAI function with LangChain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0746556627ee522"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: openai function example with langchain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "123013e1d2313ce6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
